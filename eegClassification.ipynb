{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = True\n",
    "\n",
    "if not local:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')#, force_remount=True)\n",
    "    %cd /content/drive/MyDrive/Sun/ML Shock/Final project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU.\n"
     ]
    }
   ],
   "source": [
    "votes_cols = [\n",
    "    \"seizure_vote\",\n",
    "    \"lpd_vote\",\n",
    "    \"gpd_vote\",\n",
    "    \"lrda_vote\",\n",
    "    \"grda_vote\",\n",
    "    \"other_vote\",\n",
    "]\n",
    "N_classes = len(votes_cols)\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not train_on_gpu:\n",
    "    print(\"CUDA is not available.  Training on CPU.\")\n",
    "else:\n",
    "    print(\"CUDA is available!  Training on GPU.\")\n",
    "    print(\"GPU count\", torch.cuda.device_count())\n",
    "\n",
    "if local:\n",
    "    dir = \"C:\\\\Users\\\\Amy\\\\Desktop\\\\Green_Git\\\\eegClassification\\\\\"\n",
    "    # Path to data subfolder\n",
    "    path = dir+\"data/data/\"\n",
    "    # Path to output to\n",
    "    path_out = dir+\"models/\"\n",
    "    # Path to save temporary data\n",
    "    save_path = dir+\"data/tmp/\"\n",
    "    # path to train data file\n",
    "    path_df = dir+\"sample_data\\\\\"\n",
    "else:\n",
    "    # specify paths for google colab\n",
    "    dir = \"/content/drive/MyDrive/Sun/ML Shock/Final project/\"\n",
    "    # Path to data subfolder\n",
    "    path = dir+\"data/\"\n",
    "    # Path to output to\n",
    "    path_out = dir+\"models/\"\n",
    "    # Path to save temporary data\n",
    "    save_path = dir+\"data/tmp/\"\n",
    "    # path to train data file\n",
    "    path_df = dir+\"data/\"\n",
    "\n",
    "\n",
    "# if path does not exist create it\n",
    "if not os.path.exists(path_out):\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "# import multiprocessing as cpu\n",
    "num_workers = 0  # cpu.cpu_count()  # - 1 #\n",
    "# how many samples per batch to load\n",
    "batch_size = 64  # 8  #\n",
    "# Is a test?\n",
    "test = False  # True  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pyarrow.parquet as pq\n",
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset for preprocessing the data\n",
    "\n",
    "It would be time consuming to preprocess the data every time the data is loaded. Instead, I preprocess the data once and save it in a file. This way, we can load the preprocessed data directly from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_stack(x):\n",
    "    return x.transpose(0, 1).reshape(299, 400).transpose(0, 1)\n",
    "\n",
    "\n",
    "def transpose_stack_eeg_spec(x):\n",
    "    return x.transpose(0, 1).reshape(129, 7 * 20).transpose(0, 1)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # Calculate mean and standard deviation once\n",
    "    mean = torch.mean(x)\n",
    "    std = torch.std(x)\n",
    "    # Avoid reshaping multiple times\n",
    "    x_flat = x.reshape(1, -1)\n",
    "    # Normalize using calculated mean and std\n",
    "    return (x_flat - mean) / std\n",
    "\n",
    "\n",
    "def tile(x):\n",
    "    return torch.tile(x, (3, 1, 1))\n",
    "\n",
    "\n",
    "def normalize_special(x):\n",
    "    # Use broadcasting to avoid unnecessary reshape operations\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
    "    return (x * std - mean).reshape(3, 400, 299)\n",
    "\n",
    "\n",
    "def normalize_special_eeg_spec(x):\n",
    "    # Use broadcasting to avoid unnecessary reshape operations\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
    "    return (x * std - mean).reshape(3, 7 * 20, 129)\n",
    "\n",
    "\n",
    "def min_max_scaling(x):\n",
    "    # Calculate min and max once\n",
    "    min_val = torch.min(x, dim=1).values.unsqueeze(1)\n",
    "    max_val = torch.max(x, dim=1).values.unsqueeze(1)\n",
    "\n",
    "    # Normalize using calculated min and max\n",
    "    return torch.div(x - min_val, max_val - min_val)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        data_type,\n",
    "        data_info_dict,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A custom data loader for the harmful brain activity dataset.\n",
    "        CustomDataset inherits from torch.utils.data.Dataset\n",
    "        and overrides the __len__ and __getitem__ methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dir : str\n",
    "            The directory where the data is stored.\n",
    "        data_type : str\n",
    "            The type of data to load. Must be one of ['spec', 'eeg_raw', 'eeg_spec'].\n",
    "        data_info_dict : dict\n",
    "            A dictionary containing the data information. The keys are tuples of the form\n",
    "            (data_id, item_id, offset) and the values are dictionaries containing the votes\n",
    "            for each class.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.data_type = data_type\n",
    "\n",
    "        if self.data_type not in [\"eeg_spec\", \"spec\", \"eeg_raw\"]:\n",
    "            raise ValueError(\n",
    "                \"Invalid data type provided. Must be one of ['spec', 'eeg_raw', 'eeg_spec']\"\n",
    "            )\n",
    "\n",
    "        if transform is None and self.data_type == \"spec\":\n",
    "            self.transform = (\n",
    "                torch.tensor,\n",
    "                transpose_stack,\n",
    "                normalize,\n",
    "                tile,\n",
    "                normalize_special,\n",
    "            )\n",
    "        elif transform is None and self.data_type == \"eeg_spec\":\n",
    "            self.transform = (\n",
    "                torch.tensor,\n",
    "                transpose_stack_eeg_spec,\n",
    "                normalize,\n",
    "                tile,\n",
    "                normalize_special_eeg_spec,\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        self.item_list = [k for k in data_info_dict.keys()]\n",
    "        self.data_info_dict = data_info_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.item_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.item_list[idx]\n",
    "        data_id, _, offset = item\n",
    "        path = self.data_dir + str(data_id) + \".parquet\"\n",
    "\n",
    "        data = self.preprocessing(path, offset)\n",
    "\n",
    "        class_votes = self.data_info_dict[item][\"votes\"]\n",
    "        if class_votes is None:\n",
    "            class_votes = np.array([])\n",
    "            label = np.array([])\n",
    "        else:\n",
    "            label = np.argmax(class_votes)\n",
    "\n",
    "        if self.transform:\n",
    "            for trans in self.transform:\n",
    "                data = trans(data)\n",
    "\n",
    "        return data, class_votes\n",
    "\n",
    "    def preprocessing(self, path, offset):\n",
    "        freq = 200 if \"eeg\" in self.data_type else 0.5  # Hz\n",
    "\n",
    "        data = pq.read_table(path).to_pandas()\n",
    "\n",
    "        # fill nan with zeros\n",
    "        data = data.fillna(0)\n",
    "        # enforce data type to float32\n",
    "        data = data.astype(np.float32)\n",
    "\n",
    "        collected_data = []\n",
    "        if self.data_type == \"spec\":\n",
    "            for spec_type in [\"LL\", \"RL\", \"LP\", \"RP\"]:\n",
    "                collected_data.append(data.filter(regex=spec_type).values)\n",
    "        elif self.data_type in [\"eeg_raw\", \"eeg_spec\"]:\n",
    "            for col in data.columns:\n",
    "                collected_data.append(data[col].values)\n",
    "\n",
    "        data = np.moveaxis(np.array(collected_data), 0, -1)\n",
    "\n",
    "        if self.data_type == \"spec\":\n",
    "            start = int((0 + offset) * freq)\n",
    "            end = int((600 + offset) * freq) - 1\n",
    "\n",
    "            # clip values between exp(-4) and exp(8)\n",
    "            data = np.clip(data[start:end, :], np.exp(-4), np.exp(8))\n",
    "            data = np.log(data)\n",
    "            # move last axis to first\n",
    "            data = np.moveaxis(data, -1, 0)\n",
    "        elif self.data_type == \"eeg_raw\":\n",
    "\n",
    "            # # select the central 50 seconds of the data\n",
    "            # start = int((0 + offset) * freq)\n",
    "            # end = int((49 + offset) * freq)\n",
    "            # select the central 10 seconds of the data\n",
    "            start = int((20 + offset) * freq)\n",
    "            end = int((29 + offset) * freq)\n",
    "\n",
    "            data = data[start:end]\n",
    "            # move last axis to first\n",
    "            data = np.moveaxis(data, -1, 0)\n",
    "        elif self.data_type == \"eeg_spec\":\n",
    "\n",
    "            # # select the central 50 seconds of the data\n",
    "            # start = int((0 + offset) * freq)\n",
    "            # end = int((49 + offset) * freq)\n",
    "            # select the central 10 seconds of the data\n",
    "            start = int((20 + offset) * freq)\n",
    "            end = int((29 + offset) * freq)\n",
    "\n",
    "            data = data[start:end]\n",
    "            # apply spectrogram to across all channels, axis 0\n",
    "            spec_data = []\n",
    "            for d in range(data.shape[1]):\n",
    "                _, _, Sxx = spectrogram(data[:, d], fs=freq)\n",
    "                # clip\n",
    "                Sxx = np.clip(Sxx, np.exp(-4), np.exp(8))\n",
    "                spec_data.append(np.log(Sxx))\n",
    "\n",
    "            data = np.moveaxis(np.array(spec_data), 0, -1)\n",
    "            data = np.array(spec_data)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid data type provided. Must be one of ['spec', 'eeg_raw', 'eeg_spec']\"\n",
    "            )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset for loading the saved preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetNPY(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        data_files,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.data_files = data_files\n",
    "        self.N_items = len(self.data_files)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N_items\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        idx = self.data_files[idx]\n",
    "        # read data from path, npy\n",
    "        data = np.load(self.data_path + \"images_\" + str(idx) + \".npy\")\n",
    "        class_votes = np.load(self.data_path + \"votes_\" + str(idx) + \".npy\")\n",
    "\n",
    "        if self.transform:\n",
    "            batch_size = data.shape[0]\n",
    "            for trans in self.transform:\n",
    "                data = trans(batch_size, data)\n",
    "\n",
    "        return data, class_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# define the CNN architecture\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes, batch_size=64):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.N_classes = N_classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.N_out = (\n",
    "            8\n",
    "            * (self.input_shape[1] // 2 // 2 // 2)\n",
    "            * (self.input_shape[2] // 2 // 2 // 2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.N_out, 500)\n",
    "        # linear layer (500 -> 6)\n",
    "        self.fc2 = nn.Linear(500, self.N_classes)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(x.shape[0], self.N_out)  # x.view(self.batch_size, self.N_out)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# define the CNN architecture\n",
    "class CustomCNN_eeg(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes, batch_size=64):\n",
    "        super(CustomCNN_eeg, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.N_classes = N_classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.N_out = (\n",
    "            8\n",
    "            * (self.input_shape[1] // 2 // 2 // 2)\n",
    "            * (self.input_shape[2] // 2 // 2 // 2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.N_out, 500)\n",
    "        # linear layer (500 -> 6)\n",
    "        self.fc2 = nn.Linear(500, self.N_classes)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(x.shape[0], self.N_out)  # x.view(self.batch_size, self.N_out)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomCNN_eeg_small(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes, batch_size=64):\n",
    "        super(CustomCNN_eeg_small, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.N_classes = N_classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 4, 4, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.N_out = (\n",
    "            8\n",
    "            * (self.input_shape[1] // 2 // 2 // 2)\n",
    "            * (self.input_shape[2] // 2 // 2 // 2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.N_out, 500)\n",
    "        # linear layer (500 -> 6)\n",
    "        self.fc2 = nn.Linear(500, self.N_classes)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(x.shape[0], self.N_out)  # x.view(self.batch_size, self.N_out)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# define the CNN architecture\n",
    "class TransNet_Resnet18(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes):\n",
    "        super(TransNet_Resnet18, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "\n",
    "        in_features = self.model.fc.in_features\n",
    "\n",
    "        # freeze all model parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove the last layer\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "        # a layer to go some shape (4,299,100) to (3,299,100)\n",
    "        # self.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # replace first layer with new layer\n",
    "        # self.model.conv1 = self.conv1\n",
    "\n",
    "        # add new layer\n",
    "        # self.fc = nn.Linear(in_features, self.N_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # x = F.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# define the CNN architecture\n",
    "class TransNet_Efficientnetb0(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes):\n",
    "        super(TransNet_Efficientnetb0, self).__init__()\n",
    "\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.model = models.efficientnet_b0(pretrained=True)\n",
    "        in_features = self.model.classifier[-1].in_features\n",
    "\n",
    "        # freeze all model parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # # remove the last layer\n",
    "        self.model.classifier[-1] = nn.Identity()\n",
    "\n",
    "        # a layer to go some shape (4,299,100) to (3,299,100)\n",
    "        # self.conv1 = nn.Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        # replace first layer with new layer\n",
    "        # self.model.features[0] = self.conv1\n",
    "\n",
    "        # add new layer\n",
    "        # self.fc = nn.Linear(in_features, self.N_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # x = F.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransNet_Resnet18_unfrozen(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes):\n",
    "        super(TransNet_Resnet18_unfrozen, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "\n",
    "        in_features = self.model.fc.in_features\n",
    "\n",
    "        # freeze all model parameters\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # remove the last layer\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "        # a layer to go some shape (4,299,100) to (3,299,100)\n",
    "        # self.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # replace first layer with new layer\n",
    "        # self.model.conv1 = self.conv1\n",
    "\n",
    "        # add new layer\n",
    "        self.fc = nn.Linear(in_features, self.N_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = F.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# define the CNN architecture\n",
    "class TransNet_Efficientnetb0_unfrozen(nn.Module):\n",
    "    def __init__(self, input_shape, N_classes):\n",
    "        super(TransNet_Efficientnetb0_unfrozen, self).__init__()\n",
    "\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.model = models.efficientnet_b0(pretrained=True)\n",
    "        in_features = self.model.classifier[-1].in_features\n",
    "\n",
    "        # freeze all model parameters\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # # remove the last layer\n",
    "        self.model.classifier[-1] = nn.Identity()\n",
    "\n",
    "        # a layer to go some shape (4,299,100) to (3,299,100)\n",
    "        # self.conv1 = nn.Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        # replace first layer with new layer\n",
    "        # self.model.features[0] = self.conv1\n",
    "\n",
    "        # add new layer\n",
    "        self.fc = nn.Linear(in_features, self.N_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = F.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all the model architectures\n",
    "# module = __import__(\"model_architectures\")\n",
    "\n",
    "\n",
    "get_batch_transform = lambda x, y: (\n",
    "    x[0, :],\n",
    "    y[0, :],\n",
    ")\n",
    "\n",
    "\n",
    "def get_data_info(df, data_type):\n",
    "\n",
    "    # train is true id votes_cols are available\n",
    "    train = all([col in df.columns for col in votes_cols])\n",
    "\n",
    "    label_cols = (\n",
    "        [\"eeg_id\", \"label_id\", \"eeg_label_offset_seconds\"]\n",
    "        if \"eeg\" in data_type\n",
    "        else [\"spectrogram_id\", \"label_id\", \"spectrogram_label_offset_seconds\"]\n",
    "    )\n",
    "    offset = (\n",
    "        [\"eeg_label_offset_seconds\"]\n",
    "        if \"eeg\" in data_type\n",
    "        else [\"spectrogram_label_offset_seconds\"]\n",
    "    )\n",
    "\n",
    "    # if info_cols not in df add it and set to zero\n",
    "    for col in offset:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    # if df does not contain \"label_id\" add a unique label_id\n",
    "    if \"label_id\" not in df.columns:\n",
    "        df[\"label_id\"] = range(len(df))\n",
    "\n",
    "    info = {}\n",
    "    df_gr = df.groupby(label_cols)\n",
    "    for name, group in df_gr:\n",
    "        # first row of group\n",
    "        info[name] = {\"votes\": group[votes_cols].values[0] if train else None}\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def shuffle(info):\n",
    "    info_shuffled = {}\n",
    "    keys = [k for k in info.keys()]\n",
    "    np.random.shuffle(keys)\n",
    "    for key in keys:\n",
    "        info_shuffled[key] = info[key]\n",
    "\n",
    "    return info_shuffled\n",
    "\n",
    "\n",
    "def egg_spec_augmentation(info, rates=[0.75, 0.2, 0.2, 0.2, 0.2, 0.2]):\n",
    "\n",
    "    info_aug = info.copy()\n",
    "\n",
    "    offset = [-1, 1, -2, 2, -3, 3, -4, 4]\n",
    "    keys = [k for k in info_aug.keys()]\n",
    "    N_item = len(keys)\n",
    "\n",
    "    counts = [0] * 6\n",
    "    for k in info_aug.keys():\n",
    "        idx = np.argmax(info_aug[k][\"votes\"])\n",
    "        counts[idx] += 1\n",
    "\n",
    "    for key in keys:\n",
    "        # generate random between 0 and 1\n",
    "        r = np.random.rand()\n",
    "        idx = np.argmax(info_aug[key][\"votes\"])\n",
    "\n",
    "        if r < rates[idx]:\n",
    "            # select a random offset\n",
    "            off = np.random.choice(offset)\n",
    "            new_key = (key[0], key[1], key[2] + off)\n",
    "            # if new_key not in info add it\n",
    "            if new_key not in info_aug.keys():\n",
    "                info_aug[new_key] = info_aug[key]\n",
    "\n",
    "            counts[idx] += 1\n",
    "\n",
    "        # if len(info) >= N_item*(1+0.4):\n",
    "        #     break\n",
    "\n",
    "    return info_aug\n",
    "\n",
    "\n",
    "def lrfn(\n",
    "    epoch,\n",
    "    epochs,\n",
    "    mode=\"cos\",\n",
    "    lr_start=2e-4,\n",
    "    lr_max=3e-5 * 64,\n",
    "    lr_min=1e-5,\n",
    "    lr_ramp_ep=4,\n",
    "    lr_sus_ep=0,\n",
    "    lr_decay=0.75,\n",
    "):\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "    elif mode == \"exp\":\n",
    "        lr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "    elif mode == \"step\":\n",
    "        lr = lr_max * lr_decay ** ((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "    elif mode == \"cos\":\n",
    "        decay_total_epochs, decay_epoch_index = (\n",
    "            epochs - lr_ramp_ep - lr_sus_ep + 3,\n",
    "            epoch - lr_ramp_ep - lr_sus_ep,\n",
    "        )\n",
    "        phase = np.pi * decay_epoch_index / decay_total_epochs\n",
    "        lr = (lr_max - lr_min) * 0.5 * (1 + np.cos(phase)) + lr_min\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train_func(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    path_model_out,\n",
    "    n_epochs,\n",
    "    learning_rate,\n",
    "    label_smoothing,\n",
    "    test=False,\n",
    "    valid_loss_min=np.Inf,\n",
    "    loss_type = \"cross_entropy\",\n",
    "):\n",
    "    \n",
    "    if loss_type == \"cross_entropy\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss_type == \"kldiv\":\n",
    "        criterion = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "    else:\n",
    "        raise ValueError(\"Loss not found\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    track_loss = []\n",
    "    track_loss_val = []\n",
    "\n",
    "    max_samples = 1\n",
    "\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate[epoch - 1]  # lrfn(epoch, n_epochs)\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        count = 0\n",
    "\n",
    "        for data, votes in tqdm(train_loader):\n",
    "            if test and count >= max_samples:\n",
    "                break\n",
    "\n",
    "            data, votes = get_batch_transform(data, votes)\n",
    "            # offset vote by adding label smoothing as offset\n",
    "            votes = votes * (1 - label_smoothing) + label_smoothing / N_classes\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, votes = data.cuda(), votes.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            if torch.isnan(output).sum() > 0:\n",
    "                print(\"Nan in output\")\n",
    "            else:\n",
    "\n",
    "                # loss\n",
    "                if loss_type == \"cross_entropy\":\n",
    "                    loss = criterion(torch.exp(output), torch.argmax(votes, axis=1))\n",
    "                elif loss_type == \"kldiv\":\n",
    "                    loss = criterion(output.float(), F.log_softmax(votes.float(), dim=1))\n",
    "                else:\n",
    "                    raise ValueError(\"Loss not found\")\n",
    "\n",
    "                # update training loss\n",
    "                train_loss += loss.item()  # *data.size(0)\n",
    "\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        train_loss = train_loss / count  # len(train_loader)#/batch_size\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for data, votes in tqdm(valid_loader):\n",
    "            if test and count >= max_samples:\n",
    "                break\n",
    "\n",
    "            data, votes = get_batch_transform(data, votes)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, votes = data.cuda(), votes.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            if torch.isnan(output).sum() > 0:\n",
    "                print(\"Nan in output\")\n",
    "            else:\n",
    "                # calculate the batch loss\n",
    "                if loss_type == \"cross_entropy\":\n",
    "                    loss = criterion(torch.exp(output), torch.argmax(votes, axis=1))\n",
    "                elif loss_type == \"kldiv\":\n",
    "                    loss = criterion(output.float(), F.log_softmax(votes.float(), dim=1))\n",
    "                else:\n",
    "                    raise ValueError(\"Loss not found\")\n",
    "\n",
    "                # update average validation loss\n",
    "                valid_loss += loss.item()  # *data.size(0)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # calculate average losses\n",
    "\n",
    "        valid_loss = valid_loss / count  # len(valid_loader)#/batch_size\n",
    "\n",
    "        track_loss.append(train_loss)\n",
    "        track_loss_val.append(valid_loss)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        print(\"{}; \\t{:.6f}; \\t{:.6f}\".format(epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), path_model_out)\n",
    "            print(\n",
    "                \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                    valid_loss_min, valid_loss\n",
    "                )\n",
    "            )\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    return track_loss, track_loss_val\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    path,\n",
    "    df,\n",
    "    train_p,\n",
    "    test_p,\n",
    "    data_type,\n",
    "    min_votes,\n",
    "    augmentation,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "):\n",
    "    data_dir = f\"train_eegs/\" if \"eeg\" in data_type else f\"train_spectrograms/\"\n",
    "    data_dir = path + data_dir\n",
    "\n",
    "    df_test = df[df[\"patient_id\"].isin(test_p)]\n",
    "    print(\"Fetching info...\")\n",
    "    info_test = get_data_info(df_test, data_type)\n",
    "\n",
    "    dataset_test = CustomDataset(data_dir, data_type, info_test)\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # split train_p into train and validation\n",
    "    split = int(0.8 * len(train_p))\n",
    "    train_p, valid_p = train_p[:split], train_p[split:]\n",
    "\n",
    "    df_train = df[df[\"patient_id\"].isin(train_p)]\n",
    "    df_train = df_train[df_train[\"total_votes\"] >= min_votes]\n",
    "    info_train = get_data_info(df_train, data_type)\n",
    "    info_train = shuffle(info_train)\n",
    "\n",
    "    if augmentation:\n",
    "        info_train_aug = egg_spec_augmentation(info_train)\n",
    "    else:\n",
    "        info_train_aug = info_train.copy()\n",
    "\n",
    "    df_valid = df[df[\"patient_id\"].isin(valid_p)]\n",
    "    # df_valid = df_valid[df_valid[\"total_votes\"] >= min_votes]\n",
    "    info_valid = get_data_info(df_valid, data_type)\n",
    "    info_valid = shuffle(info_valid)\n",
    "\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_dataset = CustomDataset(data_dir, data_type, info_train_aug)\n",
    "    valid_dataset = CustomDataset(data_dir, data_type, info_valid)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "def run(\n",
    "    path_out,\n",
    "    model_name,\n",
    "    label_smoothing,\n",
    "    input_shape,\n",
    "    n_epochs,\n",
    "    model_info,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    test_loader,\n",
    "    is_test=False,\n",
    "    loss_type=\"cross_entropy\",\n",
    "):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "    # get model name as executable function\n",
    "    # model = getattr(module, model_name)(input_shape=input_shape, N_classes=N_classes)\n",
    "    model = globals()[model_name](input_shape=input_shape, N_classes=N_classes)\n",
    "\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    # print(\"Number of parameters in the model\", num_parameters)\n",
    "    model_info[\"num_parameters\"] = num_parameters\n",
    "\n",
    "    # number of trainable parameters\n",
    "    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # print(\"Number of trainable parameters in the model\", num_parameters)\n",
    "    model_info[\"num_trainable_parameters\"] = num_parameters\n",
    "\n",
    "    # check if \"f\"./model_{model_name}_*\" exists and add 1 to the index\n",
    "    index = 0\n",
    "    model_path = path_out + \"model_{model_name}_{index}.pt\"\n",
    "    while os.path.exists(model_path.format(model_name=model_name, index=index)):\n",
    "        index += 1\n",
    "\n",
    "    configs = {\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"path_model_out\": path_out + f\"model_{model_name}_{index}.pt\",\n",
    "        \"learning_rate\": [lrfn(epoch, n_epochs) for epoch in np.arange(n_epochs)],\n",
    "        \"label_smoothing\": label_smoothing,\n",
    "        \"loss_type\": loss_type,\n",
    "    }\n",
    "\n",
    "    model_info[\"configs\"] = configs\n",
    "    model_info[\"model_name\"] = model_name\n",
    "    model_info[\"input_shape\"] = input_shape\n",
    "\n",
    "    valid_loss_min = (\n",
    "        np.min(model_info[\"track_loss_val\"])\n",
    "        if len(model_info[\"track_loss_val\"]) > 0\n",
    "        else np.Inf\n",
    "    )\n",
    "\n",
    "    track_loss, track_loss_val = train_func(\n",
    "        model,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        valid_loss_min=valid_loss_min,\n",
    "        test=is_test,\n",
    "        **configs,\n",
    "    )\n",
    "\n",
    "    model_info[\"track_loss\"] += track_loss\n",
    "    model_info[\"track_loss_val\"] += track_loss_val\n",
    "\n",
    "    model.load_state_dict(torch.load(configs[\"path_model_out\"]))\n",
    "\n",
    "    if loss_type == \"cross_entropy\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss_type == \"kldiv\":\n",
    "        criterion = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "    else:\n",
    "        raise ValueError(\"Loss not found\")\n",
    "\n",
    "\n",
    "    if test_loader is not None:\n",
    "\n",
    "        # track test loss\n",
    "        test_loss = 0.0\n",
    "        test_loss_baseline = 0.0\n",
    "        class_correct = list(0.0 for i in range(N_classes))\n",
    "        class_total = list(0.0 for i in range(N_classes))\n",
    "        model.eval()\n",
    "\n",
    "        max_samples = 1\n",
    "        cm_y_pred = []\n",
    "        cm_y_true = []\n",
    "        # iterate over test data\n",
    "        count = 0\n",
    "        for data, votes in tqdm(test_loader):\n",
    "            if is_test and count >= max_samples:\n",
    "                break\n",
    "\n",
    "            data, votes = get_batch_transform(data, votes)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, votes = data.cuda(), votes.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            if torch.isnan(output).sum() > 0:\n",
    "                print(\"Nan in output\")\n",
    "            else:\n",
    "                count += 1\n",
    "                # calculate the batch loss\n",
    "                if loss_type == \"cross_entropy\":\n",
    "                    loss = criterion(torch.exp(output), torch.argmax(votes, axis=1))\n",
    "                elif loss_type == \"kldiv\":\n",
    "                    loss = criterion(output.float(), F.log_softmax(votes.float(), dim=1))\n",
    "                else:\n",
    "                    raise ValueError(\"Loss not found\")\n",
    "\n",
    "                test_loss += loss.item()  # *data.size(0)\n",
    "\n",
    "                # dummy is a tensor filled with 1/6 of shape [64,6]\n",
    "                dummy = torch.ones(data.size(0), N_classes).to(device)\n",
    "                dummy = dummy / N_classes\n",
    "                if loss_type == \"cross_entropy\":\n",
    "                    loss_baseline = criterion(dummy, torch.argmax(votes, axis=1))\n",
    "                elif loss_type == \"kldiv\":\n",
    "                    loss_baseline = criterion(\n",
    "                        F.log_softmax(dummy, dim=1), F.log_softmax(votes.float(), dim=1)\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\"Loss not found\")\n",
    "\n",
    "                test_loss_baseline += loss_baseline.item()\n",
    "\n",
    "                # convert output probabilities to predicted class\n",
    "                _, pred = torch.max(output, 1)\n",
    "                # compare predictions to true label\n",
    "                target = torch.argmax(votes, axis=1)\n",
    "                correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                correct = (\n",
    "                    np.squeeze(correct_tensor.numpy())\n",
    "                    if not train_on_gpu\n",
    "                    else np.squeeze(correct_tensor.cpu().numpy())\n",
    "                )\n",
    "                # calculate test accuracy for each object class\n",
    "                for i in range(target.shape[0]):\n",
    "                    label = target.data[i]\n",
    "                    class_correct[label] += correct[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "                    cm_y_pred.append(pred[i].item())\n",
    "                    cm_y_true.append(target.data[i].item())\n",
    "\n",
    "        # average test loss\n",
    "        test_loss = test_loss / count  # len(test_loader.dataset)  # /batch_size\n",
    "        print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "        test_loss_baseline = (\n",
    "            test_loss_baseline / count\n",
    "        )  # len(test_loader.dataset)  # /batch_size\n",
    "        print(\"Test Loss Baseline: {:.6f}\\n\".format(test_loss_baseline))\n",
    "\n",
    "        cm = confusion_matrix(cm_y_true, cm_y_pred)\n",
    "        cm_p = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        model_info[\"test_loss\"] = test_loss\n",
    "        model_info[\"test_loss_baseline\"] = test_loss_baseline\n",
    "        model_info[\"confusion_matrix\"] = cm.tolist()\n",
    "        model_info[\"confusion_matrix_p\"] = cm_p.tolist()\n",
    "\n",
    "\n",
    "def save_data(loaders, names, save_path, test=False):\n",
    "    for data_loader, text in zip(\n",
    "        loaders,\n",
    "        names,\n",
    "    ):\n",
    "        tmp_path = save_path + f\"{text}/\"\n",
    "        if not os.path.exists(tmp_path):\n",
    "            os.makedirs(tmp_path)\n",
    "\n",
    "        count = 0\n",
    "        for X, votes in tqdm(data_loader, desc=f\"Saving {text} data\"):\n",
    "\n",
    "            if test and count >= 3:\n",
    "                break\n",
    "\n",
    "            # save the images\n",
    "            np.save(tmp_path + f\"images_{count}.npy\", X.numpy())\n",
    "            # save the votes\n",
    "            np.save(tmp_path + f\"votes_{count}.npy\", votes.numpy())\n",
    "            count += 1\n",
    "\n",
    "\n",
    "def run_inference(model_name, path_model_out, test_loader, input_shape, is_test=False):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "    # get model name as executable function\n",
    "    # model = getattr(module, model_name)(input_shape=input_shape, N_classes=N_classes)\n",
    "    model = globals()[model_name](input_shape=input_shape, N_classes=N_classes)\n",
    "    # load trained model\n",
    "    model.load_state_dict(torch.load(path_model_out))\n",
    "\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # track test loss\n",
    "    test_loss = 0.0\n",
    "    test_loss_baseline = 0.0\n",
    "    class_correct = list(0.0 for i in range(N_classes))\n",
    "    class_total = list(0.0 for i in range(N_classes))\n",
    "    model.eval()\n",
    "\n",
    "    max_samples = 1\n",
    "    cm_y_pred = []\n",
    "    cm_y_true = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    # iterate over test data\n",
    "    count = 0\n",
    "    for data, votes in tqdm(test_loader):\n",
    "        if is_test and count >= max_samples:\n",
    "            break\n",
    "\n",
    "        data, votes = get_batch_transform(data, votes)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, votes = data.cuda(), votes.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "\n",
    "        if torch.isnan(output).sum() > 0:\n",
    "            print(\"Nan in output\")\n",
    "        else:\n",
    "            count += 1\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output.float(), F.log_softmax(votes.float(), dim=1))\n",
    "            # loss = criterion(torch.exp(output), torch.argmax(votes, axis=1))\n",
    "\n",
    "            test_loss += loss.item()  # *data.size(0)\n",
    "\n",
    "            # dummy is a tensor filled with 1/6 of shape [64,6]\n",
    "            dummy = torch.ones(data.size(0), N_classes).to(device)\n",
    "            dummy = dummy / N_classes\n",
    "            loss_baseline = criterion(\n",
    "                F.log_softmax(dummy, dim=1), F.log_softmax(votes.float(), dim=1)\n",
    "            )\n",
    "            # loss_baseline = criterion(dummy, torch.argmax(votes, axis=1))\n",
    "\n",
    "            test_loss_baseline += loss_baseline.item()\n",
    "\n",
    "            # convert output probabilities to predicted class\n",
    "            _, pred = torch.max(output, 1)\n",
    "            # compare predictions to true label\n",
    "            target = torch.argmax(votes, axis=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = (\n",
    "                np.squeeze(correct_tensor.numpy())\n",
    "                if not train_on_gpu\n",
    "                else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            )\n",
    "            # calculate test accuracy for each object class\n",
    "            for i in range(target.shape[0]):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "                cm_y_pred.append(pred[i].item())\n",
    "                cm_y_true.append(target.data[i].item())\n",
    "\n",
    "        predictions.append(output.cpu().detach().numpy())\n",
    "        labels.append(votes.cpu().detach().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    accuracy = 100 * np.sum(class_correct) / np.sum(class_total)\n",
    "    print(\"Test Accuracy: {:.6f}%\\n\".format(accuracy))\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss / count  # len(test_loader.dataset)  # /batch_size\n",
    "    print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "    test_loss_baseline = (\n",
    "        test_loss_baseline / count\n",
    "    )  # len(test_loader.dataset)  # /batch_size\n",
    "    print(\"Test Loss Baseline: {:.6f}\\n\".format(test_loss_baseline))\n",
    "\n",
    "    cm = confusion_matrix(cm_y_true, cm_y_pred)\n",
    "    cm_p = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    return accuracy, test_loss, test_loss_baseline, cm, cm_p, predictions, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the training data info and load or create the data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test split\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_df + f\"train.csv\")\n",
    "\n",
    "df[\"total_votes\"] = df[\n",
    "    [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "# if path_out + \"split.json\" exist load it\n",
    "if os.path.exists(path_out + \"split.json\"):\n",
    "    with open(path_out + \"split.json\", \"r\") as f:\n",
    "        split = json.load(f)\n",
    "    train_p = np.array(split[\"train\"])\n",
    "    test_p = np.array(split[\"test\"])\n",
    "    print(\"Loaded train/test split\")\n",
    "else:\n",
    "    p_id = df[\"patient_id\"].unique()\n",
    "    np.random.shuffle(p_id)\n",
    "    # test train split\n",
    "    train_p = p_id[: int(0.8 * len(p_id))]\n",
    "    test_p = p_id[int(0.8 * len(p_id)) :]\n",
    "    print(\"Created train/test split\")\n",
    "\n",
    "    # record the split in dict and save to json\n",
    "    split = {\"train\": train_p.tolist(), \"test\": test_p.tolist()}\n",
    "    with open(path_out + \"split.json\", \"w\") as f:\n",
    "        json.dump(split, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter data, eg number of votes >5\n",
    "# min_votes = [0, 5]\n",
    "# # augment data (window shifting)\n",
    "# augmentation = [False, True]\n",
    "# # label smoothing\n",
    "# label_smoothing = [0, 0.01]\n",
    "# # number of epochs to train the model\n",
    "# n_epochs = 40 if not test else 1\n",
    "# # data type\n",
    "# data_type = \"eeg_spec\"  \n",
    "# # model name\n",
    "# model_name = \"CustomCNN_eeg\"\n",
    "# input_shape = (3, 140, 129)\n",
    "# transform = tuple()\n",
    "\n",
    "# # filter data for #votes >5\n",
    "# min_votes = [0, 5]\n",
    "# # augment data\n",
    "# augmentation = [False]\n",
    "# # label smoothing\n",
    "# label_smoothing = [0, 0.01]\n",
    "# # number of epochs to train the model\n",
    "# n_epochs = 20 if not test else 1\n",
    "# # data type\n",
    "# data_type = \"spec\"  \n",
    "# model_name = \"CustomCNN\"\n",
    "# input_shape = (3, 400, 299)\n",
    "\n",
    "# filter data, eg number of votes >5\n",
    "min_votes = [0, 5]\n",
    "# augment data (window shifting)\n",
    "augmentation = [True, False]\n",
    "# label smoothing\n",
    "label_smoothing = [0, 0.01]\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10 if not test else 1\n",
    "# data type\n",
    "data_type = \"eeg_spec\"  \n",
    "# model name\n",
    "model_name = \"CustomCNN_eeg_small\"\n",
    "input_shape = (1, 140, 129)\n",
    "# select one of the 3 channels\n",
    "transform = (lambda batch_size, x: x[:, 0, :, :].reshape(batch_size, *input_shape),)\n",
    "\n",
    "\n",
    "# # filter data, eg number of votes >5\n",
    "# min_votes = [0, 5]\n",
    "# # augment data (window shifting)\n",
    "# augmentation = [False, True]\n",
    "# # label smoothing\n",
    "# label_smoothing = [0, 0.01]\n",
    "# # number of epochs to train the model\n",
    "# n_epochs = 8 if not test else 1\n",
    "# # data type\n",
    "# data_type = \"eeg_spec\"  # \"eeg_raw\" #\"spec\" #\n",
    "# # model name\n",
    "# model_name = \"TransNet_Resnet18_unfrozen\"\n",
    "# input_shape = (3, 140, 129)\n",
    "# transform = tuple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training loop for the different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eeg_spec'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching info...\n",
      "Creating data loaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving train_eeg_spec data:   8%|▊         | 117/1395 [02:13<27:23,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "for mv in min_votes:\n",
    "    for aug in augmentation:\n",
    "\n",
    "        train_loader, valid_loader, test_loader = create_data_loaders(\n",
    "            path, df, train_p, test_p, data_type, mv, aug, batch_size, num_workers\n",
    "        )\n",
    "\n",
    "        if first:\n",
    "            save_data(\n",
    "                [train_loader, valid_loader, test_loader],\n",
    "                [f\"train_{data_type}\", f\"valid_{data_type}\", f\"test_{data_type}\"],\n",
    "                save_path,\n",
    "                test,\n",
    "            )\n",
    "            first = False\n",
    "        else:\n",
    "            save_data([train_loader, valid_loader], [f\"train_{data_type}\", f\"valid_{data_type}\"], save_path, test)\n",
    "\n",
    "        train_data = CustomDatasetNPY(\n",
    "            save_path + f\"train_{data_type}/\",\n",
    "            [str(i) for i in range(len(train_loader))],\n",
    "            transform=transform,\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_data, batch_size=1, shuffle=False, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        valid_data = CustomDatasetNPY(\n",
    "            save_path + f\"valid_{data_type}/\",\n",
    "            [str(i) for i in range(len(valid_loader))],\n",
    "            transform=transform,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_data, batch_size=1, shuffle=False, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        # test_data = CustomDatasetNPY(\n",
    "        #     save_path + f\"test_{data_type}/\",\n",
    "        #     [str(i) for i in range(len(test_loader))],\n",
    "        #     transform=transform,\n",
    "        # )\n",
    "        # test_loader = DataLoader(\n",
    "        #     test_data, batch_size=1, shuffle=False, num_workers=num_workers\n",
    "        # )\n",
    "\n",
    "        test_loader = None\n",
    "\n",
    "        for ls in label_smoothing:\n",
    "            model_info = {}\n",
    "            model_info[\"track_loss\"] = []\n",
    "            model_info[\"track_loss_val\"] = []\n",
    "            model_info[\"min_votes\"] = mv\n",
    "            model_info[\"augmentation\"] = aug\n",
    "            model_info[\"data_type\"] = data_type\n",
    "\n",
    "            run(\n",
    "                path_out,\n",
    "                model_name,\n",
    "                ls,\n",
    "                input_shape,\n",
    "                n_epochs,\n",
    "                model_info,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                test_loader,\n",
    "                is_test=test,\n",
    "            )\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # save model_info as json\n",
    "            model_info_path = model_info[\"configs\"][\"path_model_out\"].replace(\n",
    "                \".pt\", \".json\"\n",
    "            )\n",
    "            with open(model_info_path, \"w\") as f:\n",
    "                json.dump(model_info, f)\n",
    "\n",
    "            print(\"Done\")\n",
    "            \n",
    "        # permanently delete all files and folders in the train and valid tmp data folder\n",
    "        for folder in [f\"train_{data_type}\", f\"valid_{data_type}\"]:\n",
    "            for file in os.listdir(save_path + folder):\n",
    "                os.remove(save_path + folder + file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = create_data_loaders(\n",
    "            path, df, train_p, test_p, data_type, 0, False, batch_size, num_workers\n",
    ")\n",
    "save_data(\n",
    "    [train_loader, valid_loader, test_loader],\n",
    "    [f\"train_{data_type}\", f\"valid_{data_type}\", f\"test_{data_type}\"],\n",
    "    save_path,\n",
    "    test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load performance metrics and plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final all json files of the form path_out + \"model_*_*.pt\"\n",
    "model_info_paths = glob.glob(path_out + \"model_*_*.json\")\n",
    "print(f\"Found {len(model_info_paths)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, file in enumerate(model_info_paths):\n",
    "    with open(file, \"r\") as f:\n",
    "        model_info = json.load(f)\n",
    "    \n",
    "    epochs = len(model_info[\"track_loss\"])\n",
    "    # plot \"track_loss\" and \"track_loss_val\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([i for i in range(epochs)], model_info[\"track_loss\"], label=\"train\")\n",
    "    ax.plot([i for i in range(epochs)], model_info[\"track_loss_val\"], label=\"valid\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"Loss: {model_info['model_name']} {index}\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_out + \"split.json\", \"r\") as f:\n",
    "    split = json.load(f)\n",
    "train_p = np.array(split[\"train\"])\n",
    "test_p = np.array(split[\"test\"])\n",
    "\n",
    "df = pd.read_csv(path_df + f\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move validation data to the training data folder for inference on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # move all files from save_path + \"valid_data\" to save_path + \"train\"\n",
    "# import shutil\n",
    "\n",
    "# valid_files = glob.glob(save_path + \"valid/*\")\n",
    "# for file in valid_files:\n",
    "#     shutil.move(file, save_path + \"train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(model_info_paths)):\n",
    "    model_info_path = model_info_paths[index]\n",
    "\n",
    "    #load model info from json\n",
    "    with open(model_info_path, \"r\") as f:\n",
    "        model_info = json.load(f)\n",
    "\n",
    "    date_type = model_info[\"data_type\"]\n",
    "    input_shape = model_info[\"input_shape\"]\n",
    "    model_name = model_info[\"model_name\"]\n",
    "    if model_name == \"CustomCNN_eeg_small\":\n",
    "        transform = (lambda batch_size, x: x[:, 0, :, :].reshape(batch_size, *input_shape),)\n",
    "    else:\n",
    "        transform = tuple()\n",
    "\n",
    "\n",
    "    N_samples = len(df[df[\"patient_id\"].isin(test_p)])\n",
    "    N_samples = np.ceil(N_samples / batch_size).astype(int)\n",
    "    test_data = CustomDatasetNPY(\n",
    "        save_path + f\"test_{data_type}/\",\n",
    "        [str(i) for i in range(N_samples)],\n",
    "        transform=transform,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_data, batch_size=1, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    path_model = path_out + f\"model_{model_name}_{index}.pt\"\n",
    "\n",
    "    print(f\"Running inference for {model_name}_{index} on test set...\")\n",
    "    accuracy, test_loss, test_loss_baseline, cm, cm_p, predictions, labels = (\n",
    "        run_inference(model_name, path_model, test_loader, input_shape, is_test=test)\n",
    "    )\n",
    "\n",
    "    #####\n",
    "\n",
    "    # save results\n",
    "\n",
    "    loss_dict = {\n",
    "        \"test_loss_kl\": test_loss,\n",
    "        \"test_loss_baseline_kl\": test_loss_baseline,\n",
    "        \"test_accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "    # save predictions and labels as npy\n",
    "    np.save(path_out + f\"test_predictions_{model_name}_{index}.npy\", predictions)\n",
    "    np.save(path_out + f\"test_labels_{model_name}_{index}.npy\", labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "    plt.yticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix: {model_name}_{index}\")\n",
    "\n",
    "    # plt.savefig(path_out + f\"cm_{model_name}_{index}.png\")\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm_p * 100, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "    plt.xticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "    plt.yticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix: {model_name}_{index}\")\n",
    "\n",
    "    # plt.savefig(path_out + f\"cm_probs_{model_name}_{index}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # print accuracy, test_loss, test_loss_baseline\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Loss Baseline: {test_loss_baseline}\")\n",
    "\n",
    "\n",
    "    # Inference on training set\n",
    "    N_samples = len(os.listdir(save_path + f\"train_{data_type}/\"))//2\n",
    "    train_data = CustomDatasetNPY(\n",
    "        save_path + f\"train_{data_type}/\",\n",
    "        [str(i) for i in range(N_samples)],\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=1, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    print(f\"Running inference for {model_name}_{index} on train set...\")\n",
    "    _, _, _, _, _, predictions, labels = (\n",
    "        run_inference(model_name, path_model, train_loader, input_shape, is_test=test)\n",
    "    )\n",
    "\n",
    "    # save predictions and labels as npy\n",
    "    np.save(path_out + f\"train_predictions_{model_name}_{index}.npy\", predictions)\n",
    "    np.save(path_out + f\"train_labels_{model_name}_{index}.npy\", labels)\n",
    "\n",
    "\n",
    "    # save loss_dict to json\n",
    "    with open(path_out + f\"eval_{model_name}_{index}.json\", \"w\") as f:\n",
    "        json.dump(loss_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir(path_out)\n",
    "data_features = [f for f in data_files if f.startswith('train_predictions')]\n",
    "data_votes = [f for f in data_files if f.startswith('train_labels')]\n",
    "\n",
    "N_items = len(data_features)\n",
    "\n",
    "print(\"Number of items\", N_items)\n",
    "\n",
    "test_data_features = [f for f in data_files if f.startswith('test_predictions')]\n",
    "test_data_votes = [f for f in data_files if f.startswith('test_labels')]\n",
    "\n",
    "N_test_items = len(test_data_features)\n",
    "\n",
    "print(\"Number of test items\", N_test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data_features into a single numpy array\n",
    "X = []\n",
    "Y = []\n",
    "rows = []\n",
    "for i in range(N_items):\n",
    "\n",
    "    X_tmp = np.load(path_out+data_features[i])\n",
    "    Y_tmp = np.load(path_out+data_votes[i])\n",
    "    # if axis = -1 contains a nan then drop that row\n",
    "    for r in np.where(np.isnan(X_tmp))[0]:\n",
    "        rows.append(r)\n",
    "rows = np.unique(rows)\n",
    "\n",
    "for i in range(N_items):\n",
    "    # exclude rows with nan\n",
    "    X_tmp = np.load(path_out+data_features[i])\n",
    "    Y_tmp = np.load(path_out+data_votes[i])\n",
    "    X_tmp = np.delete(X_tmp, rows, axis = 0)\n",
    "    X_tmp = X_tmp.reshape(1, *X_tmp.shape)\n",
    "    Y_tmp = np.delete(Y_tmp, rows, axis = 0)\n",
    "    Y_tmp = Y_tmp.reshape(1, *Y_tmp.shape)\n",
    "    \n",
    "\n",
    "    X.append(X_tmp)\n",
    "    Y.append(Y_tmp)\n",
    "\n",
    "X = np.vstack(X)\n",
    "X= np.exp(X)\n",
    "Y = np.vstack(Y)\n",
    "Y = Y[0,:,:].reshape(1, *Y.shape[1:])\n",
    "\n",
    "N_samples = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kl divergence taking into account zeros\n",
    "def kl(p, q):\n",
    "    p = np.maximum(p, 1e-12)\n",
    "    q = np.maximum(q, 1e-12)\n",
    "    return np.mean(p * np.log(p / q))\n",
    "\n",
    "def mean_KL(Y_p, X_1, N_samples, N_classes):\n",
    "    # apply kl divergence to all time points\n",
    "    KL = np.zeros((N_samples, N_classes))\n",
    "    for i in range(N_samples):\n",
    "        KL[i] = kl(Y_p[0,i,:], X_1[i,:])\n",
    "\n",
    "    KL = np.sum(KL, axis=1)\n",
    "\n",
    "    return np.mean(KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_p = Y/np.sum(Y, axis=2).reshape(1,N_samples,1)\n",
    "\n",
    "theta = [1/N_items]*N_items\n",
    "theta_0 = np.array(theta).reshape(N_items,1,1)\n",
    "X_0 = np.sum(theta_0*X, axis=0) \n",
    "X_0 = X_0/X_0.sum(axis=-1).reshape(N_samples, 1)\n",
    "\n",
    "print(f\"{mean_KL(Y_p, X_0, N_samples, N_classes):.2f}\")\n",
    "\n",
    "KL = []\n",
    "lr = 0.001\n",
    "for _ in range(150):\n",
    "\n",
    "    grad = (np.sum(np.sum((Y_p*X)/np.sum(theta_0*X, axis=0).reshape(1,N_samples,6), axis = -1), axis = -1) -1)#\n",
    "    theta_new = (theta_0.reshape(N_items) - lr*grad).reshape(N_items,1,1)\n",
    "    \n",
    "    X_0 = np.sum(theta_new*X, axis=0) \n",
    "    X_0 = X_0/X_0.sum(axis=-1).reshape(N_samples, 1)\n",
    "    KL.append(mean_KL(Y_p, X_0, N_samples, N_classes))\n",
    "    if len(KL)>1:\n",
    "        if KL[-1]>KL[-2]:\n",
    "            break\n",
    "        else:\n",
    "            theta_0 = theta_new\n",
    "\n",
    "    # break\n",
    "\n",
    "print(f\"{mean_KL(Y_p, X_0, N_samples, N_classes):.2f}\")\n",
    "# plot the KL divergence\n",
    "plt.plot(KL, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data_features into a single numpy array\n",
    "X_test = []\n",
    "Y_test = []\n",
    "rows = []\n",
    "for i in range(N_items):\n",
    "\n",
    "    X_tmp = np.load(path_out+data_features[i])\n",
    "    Y_tmp = np.load(path_out+data_votes[i])\n",
    "    # if axis = -1 contains a nan then drop that row\n",
    "    for r in np.where(np.isnan(X_tmp))[0]:\n",
    "        rows.append(r)\n",
    "rows = np.unique(rows)\n",
    "print(rows)\n",
    "\n",
    "for i in range(N_test_items):\n",
    "    # exclude rows with nan\n",
    "    X_tmp = np.load(path_out+data_features[i])\n",
    "    Y_tmp = np.load(path_out+data_votes[i])\n",
    "    X_tmp = np.delete(X_tmp, rows, axis = 0)\n",
    "    X_tmp = X_tmp.reshape(1, *X_tmp.shape)\n",
    "    Y_tmp = np.delete(Y_tmp, rows, axis = 0)\n",
    "    Y_tmp = Y_tmp.reshape(1, *Y_tmp.shape)\n",
    "    \n",
    "\n",
    "    X_test.append(X_tmp)\n",
    "    Y_test.append(Y_tmp)\n",
    "\n",
    "X_test = np.vstack(X_test)\n",
    "X_test= np.exp(X_test)\n",
    "Y_test = np.vstack(Y_test)\n",
    "Y_test = Y[0,:,:].reshape(1, *Y_test.shape[1:])\n",
    "\n",
    "N_test_samples = X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ensemble = np.sum(theta_new*X_test, axis=0)\n",
    "X_ensemble = X_ensemble/X_ensemble.sum(axis=-1).reshape(N_test_samples, 1)\n",
    "Y_p_test = Y_test/np.sum(Y_test, axis=2).reshape(1,N_test_samples,1)\n",
    "\n",
    "KL_test = mean_KL(Y_p_test, X_ensemble, N_test_samples, N_classes)\n",
    "\n",
    "print(f\"{KL_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test_class = np.argmax(Y_p_test, axis=2)[0,:]\n",
    "predictions = np.argmax(X_ensemble, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "cm = confusion_matrix(y_test_class, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.xticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "plt.yticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Ensemble model')\n",
    "plt.show()\n",
    "\n",
    "cm_p = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm_p, annot=True, fmt=\".1f\", cmap='Blues')\n",
    "plt.xticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "plt.yticks(ticks=np.arange(6) + 0.5, labels=votes_cols)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Ensemble model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete data after run to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permanently delete all files and folders in the train and valid tmp data folder\n",
    "# for folder in [\"train/\", \"valid/\"]:\n",
    "#     for file in os.listdir(save_path + folder):\n",
    "#         os.remove(save_path + folder + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
