{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    %cd /content/drive/MyDrive/Sun/ML Shock/Final project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if local:\n",
    "    sys.path.insert(0,\"C:/Users/Amy/Desktop/Green_Git/eegClassification/utils\")\n",
    "else:\n",
    "    sys.path.insert(0,\"/content/drive/MyDrive/Sun/ML Shock/Final project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the image classes\n",
    "classes = [\n",
    "        \"seizure_vote\",\n",
    "        \"lpd_vote\",\n",
    "        \"gpd_vote\",\n",
    "        \"lrda_vote\",\n",
    "        \"grda_vote\",\n",
    "        \"other_vote\",\n",
    "    ]\n",
    "N_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of input data\n",
    "data_type = \"spec\" # \"eeg_raw\" #\"eeg_spec\"  #\n",
    "# number of subprocesses to use for data loading\n",
    "import multiprocessing as cpu\n",
    "num_workers = 0 #cpu.cpu_count() #- 1\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "model_name =   \"TransNet_Efficientnetb0\" #\"TransNet_Resnet18\" #\n",
    "input_shape =  (64,512) if model_name == \"TransNet_Resnet18\" else (64,1280)\n",
    "batch_size = input_shape[0]\n",
    "N_features = input_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (train or test) data from csv file\n",
    "if local:\n",
    "    path = f\"C:/Users/Amy/Desktop/Green_Git/eegClassification/\"\n",
    "else:\n",
    "    path = f\"./\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'features'\n",
    "data_path = path+f'data/{model_name}_{text}_{data_type}/'\n",
    "data_files = os.listdir(data_path)\n",
    "data_features = [f for f in data_files if f.startswith(text)]\n",
    "data_votes = [f for f in data_files if f.startswith('votes')]\n",
    "N_items = len(data_features)\n",
    "\n",
    "print(\"Number of items\", N_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(data_path+data_features[0]).shape, np.load(data_path+data_votes[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data_features into a single numpy array\n",
    "X = np.zeros((N_items,*input_shape))\n",
    "for i in range(N_items):\n",
    "    X[i] = np.load(data_path+data_features[i])\n",
    "\n",
    "# load all data_votes into a single numpy array\n",
    "Y = np.zeros((N_items, batch_size, N_classes))\n",
    "for i in range(N_items):\n",
    "    Y[i] = np.load(data_path+data_votes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data to (N_items*input_shape[0], input_shape[1])\n",
    "X = X.reshape(N_items*batch_size, N_features)\n",
    "Y = Y.reshape(N_items*batch_size, N_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert votes to class probabilities\n",
    "Y = Y/np.sum(Y, axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the votes\n",
    "Y_oh = np.argmax(Y, axis=1)\n",
    "# Y_oh = np.eye(N_classes)[Y_oh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost for feature importance on a classification problem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_oh, test_size=0.33, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold cross validation to find the best parameter configuration\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "objective = 'multi:softmax'\n",
    "num_class=N_classes\n",
    "model = XGBClassifier(objective=objective, \n",
    "                      num_class=num_class,\n",
    "                      )\n",
    "\n",
    "# grid search\n",
    "n_estimators = [100, 200, 300]\n",
    "learning_rate = [0.0001, 0.001, 0.01]\n",
    "max_depth = [3, 6, 9]\n",
    "param_grid = dict(learning_rate=learning_rate, \n",
    "                  n_estimators=n_estimators, \n",
    "                  max_depth=max_depth,)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with best parameters\n",
    "model = XGBClassifier(objective=objective, num_class=num_class, **grid_result.best_params_)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "filename = f\"./xgboost_{model_name}_{data_type}.sav\"\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict_proba(X_test)\n",
    "\n",
    "# save predictions to file\n",
    "filename = f\"./xgboost_predictions_{model_name}_{data_type}.npy\"\n",
    "np.save(filename, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, ', '.join([\"{0:0.2f}\".format(x) for x in y_pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(y_pred, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0], y_test_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_class, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test_class, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.xticks(ticks=np.arange(6) + 0.5, labels=classes)\n",
    "plt.yticks(ticks=np.arange(6) + 0.5, labels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0], y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using kl divergence\n",
    "from scipy.stats import entropy\n",
    "kl_div = np.mean([entropy(y_test[i], y_pred[i]) for i in range(len(y_test))])\n",
    "lk_base = np.mean([entropy(y_test[i], np.array([1/6]*6)) for i in range(len(y_test))])\n",
    "print(f\"KL div: {kl_div:.3f} (baseline: {lk_base:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
