{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# %cd /content/drive/MyDrive/Sun/ML Shock/Final project/"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["path = '../'\n","data_dir = path +'data/'\n","\n","# path = './' #'/content/drive/MyDrive/Sun/ML Shock/Final project/'\n","# data_dir = path +'preprocessing/'\n","\n","\n","data_type = 'eeg_spec'"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import pickle"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data_dir, data_type, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        all_file_list = os.listdir(data_dir)\n","        self.file_list = [f for f in all_file_list if f.endswith('.npy') and data_type in f]\n","        self.label_list = [f for f in all_file_list if f.endswith('.npy') and 'y_' in f]\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        file_path = os.path.join(self.data_dir, self.file_list[idx])\n","        label_path = os.path.join(self.data_dir, self.label_list[idx])\n","        data = np.load(file_path)\n","        class_probs = np.load(label_path)\n","        label = np.argmax(class_probs)\n","        if self.transform:\n","            for trans in self.transform:\n","                data = trans(data)\n","        return data, label, class_probs\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["# # load the scaler\n","# with open(path + f'files/{data_type}_standard_scaler.pkl', 'rb') as f:\n","#     standard_scaler = pickle.load(f)\n","\n","# with open(path + f'files/{data_type}_min_max_scaler.pkl', 'rb') as f:\n","#     min_max_scaler = pickle.load(f)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# load the data\n","training_data = CustomDataset(data_dir, data_type = data_type) #, transform = (lambda x: standard_scaler.transform(x),))\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"caCBXmh2ePSN"},"outputs":[],"source":["# X_train, y_train, id_train = get_data(get_func= get_eeg_raw_features,\n","#                                       files = files_train,\n","#                                       N = int(0.5*len(files_train)),)\n","# # original_shape = X_train.shape\n","# X_train = X_train.reshape(X_train.shape[0], -1)\n","# print('train:', X_train.shape, y_train.shape, id_train.shape)\n","# # # reshape back to original shape\n","# # X_train = X_train.reshape(original_shape)"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"zcwvUM09ePSO"},"outputs":[],"source":["# X_valid, y_valid, id_valid = get_data(get_func= get_eeg_raw_features,\n","#                                       files = files_valid,\n","#                                       N = int(0.5*len(files_valid)))\n","# X_valid = X_valid.reshape(X_valid.shape[0], -1)\n","# print('valid:', X_valid.shape, y_valid.shape, id_valid.shape)"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"-0iEmtbXePSO"},"outputs":[],"source":["# X_test, y_test, id_test = get_data(get_func= get_eeg_raw_features,\n","#                                     files = files_test,\n","#                                     N = int(0.5*len(files_test)))\n","# X_test = X_test.reshape(X_test.shape[0], -1)\n","# print('test:', X_test.shape, y_test.shape, id_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"AB3S0Uy8ePSQ"},"source":["## Error function"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"T1TSt9Y-ePSQ"},"outputs":[],"source":["from scipy.special import kl_div\n","\n","def get_error(y_true, y_pred):\n","    return np.mean(kl_div(y_true, y_pred))"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"WMUJcX_YePSQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["error for uniform predictio: 0.19\n"]}],"source":["error = []\n","count = 0\n","dummy = [1/6 for _ in range(6)]\n","for _, _, y in train_dataloader:\n","    for i in range(y.shape[0]):\n","        error.append(get_error(y[i].numpy(), dummy))\n","        count += 1\n","\n","print(f'error for uniform predictio: {sum(error)/count:.2f}')"]},{"cell_type":"markdown","metadata":{"id":"epeebrbvePSR"},"source":["## Gradient boost classifier"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# # load the pca\n","# with open(path + f'{data_type}_pca.pkl', 'rb') as f:\n","#     pca = pickle.load(f)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["# load the data\n","training_data = CustomDataset(data_dir, data_type = data_type)#, \n","                            #   transform = (lambda x: standard_scaler.transform(x), lambda x: pca.transform(x.reshape(1,-1))))\n","train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"xGN8UzCWePSS"},"outputs":[],"source":["# train a gradient boosting model to predict one of the 6 classes\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n","for X, y, _ in train_dataloader:\n","    model.fit(X.reshape(X.shape[0], -1), y)\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([14, 6]) (14, 2)\n"]},{"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (6,) (2,) ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[56], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape, y_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m----> 8\u001b[0m         error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror for the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m/\u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mget_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_error\u001b[39m(y_true, y_pred):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (2,) "]}],"source":["# compute the error on the training set\n","error = 0 \n","count = 0\n","for X, _, y in train_dataloader:\n","    y_pred = model.predict_proba(X.reshape(X.shape[0], -1))\n","    print(y.shape, y_pred.shape)\n","    for i in range(y.shape[0]):\n","        error += get_error(y[i].numpy(), y_pred[i])\n","        count += 1\n","\n","print(f'error for the model: {error/count:.2f}')"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"cWx5EpJcePSX"},"outputs":[],"source":["# # predict the class probabilities\n","# y_train_pred = model.predict_proba(X_train_pca)\n","# y_valid_pred = model.predict_proba(X_valid_pca)\n","# y_test_pred = model.predict_proba(X_test_pca)\n","\n","# # calculate the error\n","# error_train = [get_error(y_true, y_pred) for y_true, y_pred in zip(y_train, y_train_pred)]\n","# error_valid = [get_error(y_true, y_pred) for y_true, y_pred in zip(y_valid, y_valid_pred)]\n","# error_test = [get_error(y_true, y_pred) for y_true, y_pred in zip(y_test, y_test_pred)]\n","\n","# print(f'train error: {np.mean(error_train):.2f}')\n","# print(f'valid error: {np.mean(error_valid):.2f}')\n","# print(f'test error: {np.mean(error_test):.2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_DOVknDePSX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoG1llnIePSX"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
